import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import plotly.graph_objects as go
import plotly.express as px
from wordcloud import WordCloud
import networkx as nx
import jieba
import re
from collections import Counter, defaultdict
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import plotly.offline as pyo
import plotly.graph_objs as go
from plotly.subplots import make_subplots
import time
import threading
from io import BytesIO
import base64

# NLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
import os
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# NLTKãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºå®Ÿã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
def setup_japanese_font():
    """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®š"""
    # Streamlit Cloudã§ã¯æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è‹±èªãƒ•ã‚©ãƒ³ãƒˆã®ã¿ã‚’ä½¿ç”¨
    plt.rcParams['font.family'] = ['sans-serif']
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['font.size'] = 12
    plt.rcParams['axes.unicode_minus'] = False

# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’å®Ÿè¡Œ
setup_japanese_font()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¢ãƒ—ãƒª",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #ff7f0e;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .info-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 1rem 0;
    }
    .metric-card {
        background-color: white;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

class TextAnalyzer:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        # æ—¥æœ¬èªã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        japanese_stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ãŒ', 'ã§', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ', 'ã•', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã‚‚', 'ã™ã‚‹', 'ã‹ã‚‰', 'ãª', 'ã“ã¨', 'ã¨ã—ã¦', 'ã„', 'ã‚„', 'ã‚Œã‚‹', 'ãªã©', 'ãªã£', 'ãªã„', 'ã“ã®', 'ãŸã‚', 'ãã®', 'ã‚ã£', 'ã‚ˆã†', 'ã¾ãŸ', 'ãã‚Œ', 'ã¨ã„ã†', 'ã‚ã‚Š', 'ã¾ã§', 'ã‚‰ã‚Œ', 'ãªã‚‹', 'ã¸', 'ã‹', 'ã ', 'ã“ã‚Œ', 'ã§', 'ã‚', 'ã‚„', 'ã‚‰ã‚Œ', 'ãªã‚‹', 'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ãŒ', 'ã§', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ', 'ã•', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã‚‚', 'ã™ã‚‹', 'ã‹ã‚‰', 'ãª', 'ã“ã¨', 'ã¨ã—ã¦', 'ã„', 'ã‚„', 'ã‚Œã‚‹', 'ãªã©', 'ãªã£', 'ãªã„', 'ã“ã®', 'ãŸã‚', 'ãã®', 'ã‚ã£', 'ã‚ˆã†', 'ã¾ãŸ', 'ãã‚Œ', 'ã¨ã„ã†', 'ã‚ã‚Š', 'ã¾ã§', 'ã‚‰ã‚Œ', 'ãªã‚‹', 'ã¸', 'ã‹', 'ã ', 'ã“ã‚Œ', 'ã§', 'ã‚', 'ã‚„', 'ã‚‰ã‚Œ', 'ãªã‚‹'
        }
        self.stop_words.update(japanese_stop_words)
    
    def detect_language(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’æ¤œå‡º"""
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        if japanese_chars > english_chars:
            return 'japanese'
        else:
            return 'english'
    
    def tokenize_text(self, text, language='auto'):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        if language == 'auto':
            language = self.detect_language(text)
        
        if language == 'japanese':
            # æ—¥æœ¬èªã®å ´åˆã¯æ•°å­—ã¨è¨˜å·ã®ã¿é™¤å»ï¼ˆæ—¥æœ¬èªæ–‡å­—ã¯ä¿æŒï¼‰
            text = re.sub(r'[0-9ï¼-ï¼™]', '', text)
            text = re.sub(r'[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\s]', '', text)
            # æ—¥æœ¬èªã®å ´åˆã¯jiebaã‚’ä½¿ç”¨
            tokens = jieba.cut(text)
            tokens = [token for token in tokens if len(token) > 1 and token not in self.stop_words and token.strip()]
        else:
            # è‹±èªã®å ´åˆã¯å¾“æ¥é€šã‚Š
            text = re.sub(r'[^\w\s]', '', text)
            text = re.sub(r'\d+', '', text)
            # è‹±èªã®å ´åˆã¯NLTKã‚’ä½¿ç”¨
            tokens = word_tokenize(text.lower())
            tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]
        
        return tokens
    
    def extract_cooccurrence(self, texts, window_size=5):
        """å…±èµ·é–¢ä¿‚ã‚’æŠ½å‡º"""
        cooccurrence = defaultdict(int)
        
        for text in texts:
            tokens = self.tokenize_text(text)
            
            for i in range(len(tokens)):
                for j in range(i + 1, min(i + window_size + 1, len(tokens))):
                    if tokens[i] != tokens[j]:
                        pair = tuple(sorted([tokens[i], tokens[j]]))
                        cooccurrence[pair] += 1
        
        return cooccurrence
    
    def get_top_words(self, texts, top_n=50):
        """é »å‡ºå˜èªã‚’å–å¾—"""
        all_tokens = []
        for text in texts:
            tokens = self.tokenize_text(text)
            all_tokens.extend(tokens)
        
        word_freq = Counter(all_tokens)
        return dict(word_freq.most_common(top_n))

def create_wordcloud(word_freq, title="ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰"):
    """ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ä½œæˆ"""
    if not word_freq:
        return None
    
    # é…ç½®ã•ã‚ŒãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
    font_path = "NotoSansJP-VariableFont_wght.ttf"
    
    try:
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
        wordcloud = WordCloud(
            width=800, 
            height=400, 
            background_color='white',
            colormap='viridis',
            font_path=font_path,
            max_words=100,
            max_font_size=100,
            min_font_size=10,
            prefer_horizontal=0.7,
            relative_scaling=0.5,
            collocations=False
        ).generate_from_frequencies(word_freq)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title(title, fontsize=16, fontweight='bold')
        
        return fig
        
    except Exception as e:
        # ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            wordcloud = WordCloud(
                width=800, 
                height=400, 
                background_color='white',
                colormap='viridis',
                max_words=50,
                max_font_size=80,
                min_font_size=8,
                prefer_horizontal=0.8,
                relative_scaling=0.3,
                collocations=False,
                font_path=None
            ).generate_from_frequencies(word_freq)
            
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')
            ax.set_title(f"{title} (ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆç‰ˆ)", fontsize=16, fontweight='bold')
            
            return fig
        except Exception as e2:
            st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e2)}")
            return None

def create_cooccurrence_network(cooccurrence, min_weight=2, max_nodes=30):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆ"""
    try:
        if not cooccurrence:
            return None
        
        # é‡ã¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_cooccurrence = {k: v for k, v in cooccurrence.items() if v >= min_weight}
        
        if not filtered_cooccurrence:
            return None
    
        # ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’æº–å‚™
        G = nx.Graph()
        
        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        for (word1, word2), weight in filtered_cooccurrence.items():
            G.add_edge(word1, word2, weight=weight)
        
        # ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã™ãã‚‹å ´åˆã¯ä¸Šä½ã®ã‚‚ã®ã ã‘ã‚’æ®‹ã™
        if len(G.nodes()) > max_nodes:
            node_weights = defaultdict(int)
            for (word1, word2), weight in filtered_cooccurrence.items():
                node_weights[word1] += weight
                node_weights[word2] += weight
            
            top_nodes = sorted(node_weights.items(), key=lambda x: x[1], reverse=True)[:max_nodes]
            top_node_names = [node for node, _ in top_nodes]
            
            # ä¸Šä½ãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ã‚¨ãƒƒã‚¸ã®ã¿ã‚’æ®‹ã™
            edges_to_keep = [(word1, word2) for (word1, word2) in G.edges() 
                             if word1 in top_node_names and word2 in top_node_names]
            G = nx.Graph()
            for word1, word2 in edges_to_keep:
                weight = filtered_cooccurrence[(word1, word2) if word1 < word2 else (word2, word1)]
                G.add_edge(word1, word2, weight=weight)
        
        if len(G.nodes()) == 0:
            return None
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¨ˆç®—
        pos = nx.spring_layout(G, k=1, iterations=50)
        
        # Plotlyã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»
        edge_x = []
        edge_y = []
        edge_weights = []
        
        for edge in G.edges(data=True):
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            edge_weights.append(edge[2]['weight'])
        
        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=0.5, color='#888'),
            hoverinfo='none',
            mode='lines')
        
        node_x = []
        node_y = []
        node_text = []
        node_size = []
        
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            
            # ãƒãƒ¼ãƒ‰ã®è©³ç´°æƒ…å ±ã‚’ä½œæˆ
            degree = G.degree(node)
            neighbors = list(G.neighbors(node))
            neighbor_text = ', '.join(neighbors[:5])  # æœ€åˆã®5ã¤ã®éš£æ¥ãƒãƒ¼ãƒ‰
            if len(neighbors) > 5:
                neighbor_text += f'... (+{len(neighbors)-5}å€‹)'
            
            hover_text = f"""
            <b>å˜èª:</b> {node}<br>
            <b>æ¬¡æ•°:</b> {degree}<br>
            <b>éš£æ¥ãƒãƒ¼ãƒ‰:</b> {neighbor_text}
            """
            node_text.append(hover_text)
            
            # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã¯æ¬¡æ•°ã«åŸºã¥ã
            node_size.append(max(10, degree * 5))
        
        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            hoverinfo='text',
            text=node_text,
            textposition="middle center",
            marker=dict(
                showscale=True,
                colorscale='YlGnBu',
                size=node_size,
                color=[],
                line_width=2
            ),
            # ãƒãƒ¼ãƒ‰ã®ãƒ‰ãƒ©ãƒƒã‚°æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
            customdata=[node for node in G.nodes()],
            hovertemplate='%{text}<extra></extra>'
        )
        
        # ãƒãƒ¼ãƒ‰ã®è‰²ã‚’æ¬¡æ•°ã«åŸºã¥ã„ã¦è¨­å®š
        node_adjacencies = []
        for node in G.nodes():
            node_adjacencies.append(len(list(G.neighbors(node))))
        node_trace.marker.color = node_adjacencies
    
        fig = go.Figure(data=[edge_trace, node_trace])
        fig.update_layout(
            title=dict(
                text='å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼‰',
                font=dict(size=16)
            ),
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20,l=5,r=5,t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ©Ÿèƒ½ã‚’è¿½åŠ 
            dragmode='pan',  # ãƒ‰ãƒ©ãƒƒã‚°ã§ç§»å‹•
            modebar=dict(
                orientation='v',
                bgcolor='rgba(255,255,255,0.7)',
                color='black',
                activecolor='red'
            ),
            # ãƒãƒ¼ãƒ‰ã®ãƒ‰ãƒ©ãƒƒã‚°æ©Ÿèƒ½ã‚’å¼·åŒ–
            clickmode='event+select',
            selectdirection='any'
        )
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ©Ÿèƒ½ã‚’å¼·åŒ–
        fig.update_traces(
            selector=dict(type='scatter'),
            hoverinfo='text+name',
            hoverlabel=dict(
                bgcolor='white',
                font_size=12,
                font_family='Arial'
            ),
            # ãƒãƒ¼ãƒ‰ã®é¸æŠã¨ãƒ‰ãƒ©ãƒƒã‚°æ©Ÿèƒ½
            selected=dict(
                marker=dict(color='red', size=15)
            ),
            unselected=dict(
                marker=dict(opacity=0.8)
            )
        )
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ©Ÿèƒ½ã‚’å¼·åŒ–
        fig.update_layout(
            # ãƒ‰ãƒ©ãƒƒã‚°æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
            dragmode='pan',
            # é¸æŠæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
            selectdirection='any',
            # ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–
            clickmode='event+select',
            # ãƒãƒ¼ãƒ‰ã®ç·¨é›†æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
            uirevision='constant'
        )
        
        return fig
    except Exception as e:
        st.error(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None



def main():
    st.markdown('<h1 class="main-header">ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¢ãƒ—ãƒª</h1>', unsafe_allow_html=True)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    st.sidebar.markdown("## ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.sidebar.file_uploader(
        "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=['csv'],
        help="UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨å¥¨ã—ã¾ã™"
    )
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    if uploaded_file is not None:
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(uploaded_file)
            
            st.markdown('<div class="info-box">', unsafe_allow_html=True)
            st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å:** {uploaded_file.name}")
            st.markdown(f"**ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º:** {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")
            st.markdown("</div>", unsafe_allow_html=True)
            
            # ã‚«ãƒ©ãƒ é¸æŠ
            st.markdown('<h2 class="sub-header">ğŸ“‹ åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ ã®é¸æŠ</h2>', unsafe_allow_html=True)
            
            text_columns = df.select_dtypes(include=['object']).columns.tolist()
            
            if len(text_columns) == 0:
                st.error("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return
            
            selected_columns = st.multiselect(
                "åˆ†æå¯¾è±¡ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆæœ€å¤§3ã¤ï¼‰",
                text_columns,
                max_selections=3,
                default=text_columns[:min(3, len(text_columns))]
            )
            
            if not selected_columns:
                st.warning("åˆ†æå¯¾è±¡ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                return
            
            # åˆ†æè¨­å®š
            st.markdown('<h2 class="sub-header">âš™ï¸ åˆ†æè¨­å®š</h2>', unsafe_allow_html=True)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                language = st.selectbox(
                    "è¨€èªè¨­å®š",
                    ["è‡ªå‹•æ¤œå‡º", "æ—¥æœ¬èª", "è‹±èª"],
                    help="ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
                )
            
            with col2:
                min_word_length = st.slider(
                    "æœ€å°å˜èªé•·",
                    min_value=1,
                    max_value=10,
                    value=2,
                    help="ã“ã®é•·ã•æœªæº€ã®å˜èªã¯é™¤å¤–ã•ã‚Œã¾ã™"
                )
            
            with col3:
                top_n_words = st.slider(
                    "è¡¨ç¤ºã™ã‚‹å˜èªæ•°",
                    min_value=10,
                    max_value=100,
                    value=50,
                    help="é »å‡ºå˜èªã®ä¸Šä½ä½•å€‹ã‚’è¡¨ç¤ºã™ã‚‹ã‹"
                )
            
            # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®å®Ÿè¡Œ
            analyzer = TextAnalyzer()
            
            # é¸æŠã•ã‚ŒãŸã‚«ãƒ©ãƒ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            all_texts = []
            for col in selected_columns:
                texts = df[col].dropna().astype(str).tolist()
                all_texts.extend(texts)
            
            if not all_texts:
                st.error("åˆ†æå¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return
            
            # æ¤œç´¢æ©Ÿèƒ½
            st.markdown('<h2 class="sub-header">ğŸ” æ¤œç´¢æ©Ÿèƒ½</h2>', unsafe_allow_html=True)
            
            search_term = st.text_input(
                "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢",
                placeholder="ä¾‹: åˆ†æ, ãƒ‡ãƒ¼ã‚¿, ãƒ†ã‚­ã‚¹ãƒˆ",
                help="å…¥åŠ›ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢ã—ã¾ã™"
            )
            
            if search_term:
                matching_texts = []
                for text in all_texts:
                    if search_term.lower() in text.lower():
                        matching_texts.append(text)
                
                if matching_texts:
                    st.success(f"'{search_term}'ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’{len(matching_texts)}ä»¶è¦‹ã¤ã‘ã¾ã—ãŸã€‚")
                    
                    # æ¤œç´¢çµæœã®è¡¨ç¤º
                    with st.expander("æ¤œç´¢çµæœã‚’è¡¨ç¤º"):
                        for i, text in enumerate(matching_texts[:10]):  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
                            st.write(f"{i+1}. {text[:200]}{'...' if len(text) > 200 else ''}")
                        
                        if len(matching_texts) > 10:
                            st.info(f"ä»–ã«{len(matching_texts) - 10}ä»¶ã®çµæœãŒã‚ã‚Šã¾ã™ã€‚")
                else:
                    st.warning(f"'{search_term}'ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æä¸­..."):
                    # è¨€èªè¨­å®šã®é©ç”¨
                    lang_setting = 'auto'
                    if language == "æ—¥æœ¬èª":
                        lang_setting = 'japanese'
                    elif language == "è‹±èª":
                        lang_setting = 'english'
                    
                    # å˜èªé »åº¦ã®åˆ†æ
                    word_freq = analyzer.get_top_words(all_texts, top_n_words)
                    
                    # å…±èµ·é–¢ä¿‚ã®åˆ†æ
                    cooccurrence = analyzer.extract_cooccurrence(all_texts)
                    
                    # çµæœã®è¡¨ç¤º
                    st.markdown('<h2 class="sub-header">ğŸ“ˆ åˆ†æçµæœ</h2>', unsafe_allow_html=True)
                    
                    # åŸºæœ¬çµ±è¨ˆ
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("ç·ãƒ†ã‚­ã‚¹ãƒˆæ•°", len(all_texts))
                    
                    with col2:
                        st.metric("ç·å˜èªæ•°", sum(word_freq.values()))
                    
                    with col3:
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", len(word_freq))
                    
                    with col4:
                        if word_freq:
                            st.metric("å¹³å‡å˜èªé•·", f"{sum(len(word) * freq for word, freq in word_freq.items()) / sum(word_freq.values()):.1f}")
                    
                    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
                    st.markdown('<h3 class="sub-header">â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰</h3>', unsafe_allow_html=True)
                    
                    if word_freq:
                        wordcloud_fig = create_wordcloud(word_freq)
                        if wordcloud_fig:
                            st.pyplot(wordcloud_fig)
                            plt.close()
                    else:
                        st.warning("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                    
                    # é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°
                    st.markdown('<h3 class="sub-header">ğŸ“Š é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°</h3>', unsafe_allow_html=True)
                    
                    if word_freq:
                        word_df = pd.DataFrame(list(word_freq.items()), columns=['å˜èª', 'å‡ºç¾å›æ•°'])
                        word_df = word_df.sort_values('å‡ºç¾å›æ•°', ascending=False)
                        
                        fig = px.bar(
                            word_df.head(20),
                            x='å‡ºç¾å›æ•°',
                            y='å˜èª',
                            orientation='h',
                            title='ä¸Šä½20å˜èªã®å‡ºç¾å›æ•°'
                        )
                        fig.update_layout(
                            height=600,
                            title=dict(
                                text='ä¸Šä½20å˜èªã®å‡ºç¾å›æ•°',
                                font=dict(size=16)
                            )
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                    st.markdown('<h3 class="sub-header">ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</h3>', unsafe_allow_html=True)
                    
                    if cooccurrence:
                        network_fig = create_cooccurrence_network(cooccurrence)
                        if network_fig:
                            st.plotly_chart(network_fig, use_container_width=True)
                            
                            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ©Ÿèƒ½ã®èª¬æ˜
                            st.markdown('<h4 class="sub-header">ğŸ–±ï¸ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ©Ÿèƒ½</h4>', unsafe_allow_html=True)
                            st.info("""
                            **æ“ä½œæ–¹æ³•:**
                            - ğŸ–±ï¸ **ãƒ‰ãƒ©ãƒƒã‚°**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã‚’ç§»å‹•
                            - ğŸ” **ã‚ºãƒ¼ãƒ **: ãƒã‚¦ã‚¹ãƒ›ã‚¤ãƒ¼ãƒ«ã§æ‹¡å¤§ãƒ»ç¸®å°
                            - ğŸ“± **ã‚¿ãƒƒãƒ**: ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§ã‚‚æ“ä½œå¯èƒ½
                            - â„¹ï¸ **ãƒ›ãƒãƒ¼**: ãƒãƒ¼ãƒ‰ã«ãƒã‚¦ã‚¹ã‚’é‡ã­ã‚‹ã¨è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
                            - ğŸ¯ **ãƒãƒ¼ãƒ‰é¸æŠ**: ãƒãƒ¼ãƒ‰ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠ
                            - âœ‹ **ãƒãƒ¼ãƒ‰ç§»å‹•**: é¸æŠã—ãŸãƒãƒ¼ãƒ‰ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã—ã¦ç§»å‹•
                            """)
                    else:
                        st.warning("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                    
                    # è©³ç´°ãªå…±èµ·é–¢ä¿‚
                    if cooccurrence:
                        st.markdown('<h3 class="sub-header">ğŸ”— è©³ç´°ãªå…±èµ·é–¢ä¿‚</h3>', unsafe_allow_html=True)
                        
                        cooccurrence_list = [(word1, word2, weight) for (word1, word2), weight in cooccurrence.items()]
                        cooccurrence_list.sort(key=lambda x: x[2], reverse=True)
                        
                        cooccurrence_df = pd.DataFrame(
                            cooccurrence_list[:50],
                            columns=['å˜èª1', 'å˜èª2', 'å…±èµ·å›æ•°']
                        )
                        
                        st.dataframe(cooccurrence_df, use_container_width=True)
        
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    else:
        st.markdown("""
        <div class="info-box">
        <h3>ğŸ“‹ ä½¿ç”¨æ–¹æ³•</h3>
        <ol>
            <li>å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„</li>
            <li>åˆ†æå¯¾è±¡ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆæœ€å¤§3ã¤ï¼‰</li>
            <li>åˆ†æè¨­å®šã‚’èª¿æ•´ã—ã¦ãã ã•ã„</li>
            <li>ã€Œåˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„</li>
        </ol>
        
        <h3>ğŸ¯ æ©Ÿèƒ½</h3>
        <ul>
            <li>ğŸ“Š ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ</li>
            <li>ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–</li>
            <li>ğŸ¬ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½</li>
            <li>ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢</li>
            <li>ğŸ“ˆ é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°</li>
        </ul>
        
        <h3>ğŸ“ å¯¾å¿œå½¢å¼</h3>
        <ul>
            <li>CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆUTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¨å¥¨ï¼‰</li>
            <li>æ—¥æœ¬èªãƒ»è‹±èªãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œ</li>
            <li>è¤‡æ•°ã‚«ãƒ©ãƒ é¸æŠå¯èƒ½</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main() 